################################################################################
# ph_tc_process_event_loss_tables.R
# This script process the event loss tables (ELTs), containing impact data for thousands
# of synthetic tropical cyclone events affecting the Philippines. The synthetic
# (or stochastic) tropical cyclone catalogue is from STORM. The imapct estimates
# are generated from the Netherlands Red Cross 510 machine learning forecasting
# model.
# Author: Alex Saunders
# Date created: 08/07/2022
# Date modified: 19/07/2022
################################################################################


################################################################################
# Install packages
################################################################################

rm(list=ls())

packages = c("")


################################################################################
# Set up working environment
################################################################################

input_path <- "C:/Users/alexa/Documents/02_work/02_start/06_technical_advisor_work/02_countries/03_philippines/01_tc/04_510_build/04_deliverables/event_loss_tables/STORM/"
input_files <- list.files(input_path)
output_path <- "C:/Users/alexa/Documents/02_work/02_start/06_technical_advisor_work/02_countries/03_philippines/01_tc/07_r/02_outputs/"


################################################################################
# Description of inputs
################################################################################

# there are 10 each containing 1000 years of simulated tropical cyclone activity
# each ELT contains the variables:
# Mun_Code - pcode for each of the municipalities
# storm_id - storm id generated by combining Year+ Month + 1+'SN' + Storm number
# typhoon - random name generated by combining Year+ Month + 1+'SN' + Storm number+ '_' +file number
# DMG_predicted - is predicted number of completely damaged houses


################################################################################
# Create a summary of events across all files
################################################################################

# loop through input files and extract unique events
for (i in 1:length(input_files)) {
  
  input_file <- input_files[i]
  file <- gsub(".csv","",gsub("STORM_DATA_","",input_file))
  elt_raw <- read.csv(paste0(input_path, input_file))
  elt <- subset(elt_raw, select = -c(X))

  # extract unique events from ELT
  events <- as.data.frame(cbind(unique(elt$typhoon), as.numeric(unique(elt$storm_id))))
  events$file <- as.numeric(file)
  colnames(events) <- c("typhoon","storm_id","file")
  
  if (i == 1) {
    events_all <- events
    
  } else {
    events_all <- rbind(events_all, events)
    
  }
  
}

events_all # 30673 events across the 10 files
length(unique(events_all$typhoon)) # 30673 events
length(unique(events_all$storm_id)) # 18573 unique storm IDs


# add a unique numeric eventID for each event and extract its year and month
#events_all$year <- as.numeric(events_all$file) * 1000 + as.numeric(substr(events_all$typhoon, 1, 4)) # original
#events_all$month <- as.numeric(substr(events_all$typhoon, 5, 6))
events_all$year <- 1 + as.numeric(events_all$file) * 1000 + as.numeric(sub("\\_.*", "", events_all$typhoon)) # original
events_all <- events_all[order(events_all$year, events_all$typhoon),]
events_all$eventID <- seq.int(nrow(events_all))

summary(events_all$year) # years 1 to 10000
#summary(events_all$month) # months 5 to 11 (May to Nov)

# write out the event_details table for reference
write.csv(events_all, paste0(output_path, "01_events/", "event_details.csv"), row.names = FALSE)



################################################################################
# Aggregate the losses per event
################################################################################


# loop through input files and aggregate losses per event
for (i in 1:length(input_files)) {
  
  input_file <- input_files[i]
  file <- gsub(".csv","",gsub("STORM_DATA_","",input_file))
  elt_raw <- read.csv(paste0(input_path, input_file))
  elt <- subset(elt_raw, select = -c(X, storm_id))
  
  # aggregate losses by event
  losses <- aggregate(elt$DMG_predicted, by = list(typhoon = elt$typhoon), FUN = sum)
  colnames(losses) <- c("typhoon","loss_bldg")
  
  if (i == 1) {
    losses_all <- losses
    
  } else {
    losses_all <- rbind(losses_all, losses)
    
  }
  
}


# merge with the event_details table and write out table for reference
losses_all <- merge(events_all, losses_all, by = "typhoon")
losses_all <- losses_all[order(losses_all$eventID),]
write.csv(losses_all, paste0(output_path, "01_events/", "event_losses.csv"), row.names = FALSE)

sum(losses_all$loss_bldg) # 2310825572, divided by 10k years = 231,083 completely damaged buildings per year


################################################################################
# Extract the losses per event for target municipalities
################################################################################

# read list of target municipalities
mun_input_path <- "C:/Users/alexa/Documents/02_work/02_start/06_technical_advisor_work/02_countries/03_philippines/01_tc/03_data/hdx/"
mun_input_file <- "target_mun.csv"
target_mun <- read.csv(paste0(mun_input_path, mun_input_file))
target_pcode <- target_mun$admin3Pcode



# loop through input files and extract the losses per event and municipality, for 
# only the target municipalities
for (i in 1:length(input_files)) {
  
  input_file <- input_files[i]
  file <- gsub(".csv","",gsub("STORM_DATA_","",input_file))
  elt_raw <- read.csv(paste0(input_path, input_file))
  elt <- subset(elt_raw, select = -c(X, storm_id, year))
  elt_mun <- elt[which(elt$Mun_Code %in% target_pcode),]

  if (i == 1) {
    elt_mun_all <- elt_mun
    
  } else {
    elt_mun_all <- rbind(elt_mun_all, elt_mun)
    
  }
  
}

# merge to get the eventID and save off the subset ELT
elt_mun_all <- merge(events_all, elt_mun_all, by = "typhoon")
elt_mun_all <- elt_mun_all[order(elt_mun_all$eventID),]
colnames(elt_mun_all) <- c("typhoon", "storm_id", "file", "eventID", "year", "Mun_Code","loss_bldg")
elt_mun_all <- subset(elt_mun_all, select = -c(typhoon, storm_id, file))
write.csv(elt_mun_all, paste0(output_path, "01_events/", "event_losses_by_target_mun.csv"), row.names = FALSE)




################################################################################
# Aggregate the losses per event for target provinces (adm2)
################################################################################


# check unique pcodes
length(unique(elt_mun_all$Mun_Code)) # 71
length(unique(target_pcode)) # 73

# aggregate losses by event and adm2
colnames(elt_mun_all) <- c("eventID", "year", "admin3Pcode","loss_bldg")
elt_mun_all_wadm2 <- merge(elt_mun_all, subset(target_mun, select = c(admin3Pcode, admin2Pcode)), by = "admin3Pcode")


losses_by_adm2 <- aggregate(elt_mun_all_wadm2$loss_bldg, by = list(eventID = elt_mun_all_wadm2$eventID, 
                                                                   year = elt_mun_all_wadm2$year,
                                                                   admin2Pcode = elt_mun_all_wadm2$admin2Pcode), 
                                                                  FUN = sum)
colnames(losses_by_adm2) <- c("eventID","year", "admin2Pcode","loss_bldg")
losses_by_adm2 <- losses_by_adm2[order(losses_by_adm2$eventID, losses_by_adm2$admin2Pcode),]

# write out table of event loss by province for target provinces
write.csv(losses_by_adm2, paste0(output_path, "01_events/", "event_losses_by_target_prov.csv"), row.names = FALSE)




################################################################################
# Create RP loss curve for each target province
################################################################################


#losses_by_adm2 <- read.csv(paste0(output_path, "event_losses_by_target_prov.csv"))


nyears <- 10000
rp <- c(2,3,4,5,6,7,8,9,10,15,20,25,50,100,150,200,250,400,500,1000,5000,10000)
pctile <- 1 - 1/rp
ep_losses <- as.data.frame(cbind(rp, pctile))

for (j in 1:length(unique(losses_by_adm2$admin2Pcode))) {
  adm2 <- unique(losses_by_adm2$admin2Pcode)[j]
  losses_adm2 <- losses_by_adm2[which(losses_by_adm2$admin2Pcode == adm2), ]
  
  # create year losses for years 1 to 10000, both aggregate and occurrence
  year_losses <- as.data.frame(c(1:nyears))
  colnames(year_losses) <- c("year")
  agg_losses <- aggregate(losses_adm2$loss_bldg, by = list(year = losses_adm2$year), FUN = sum)
  colnames(agg_losses) <- c("year","agg")
  occ_losses <- aggregate(losses_adm2$loss_bldg, by = list(year = losses_adm2$year), FUN = max)
  colnames(occ_losses) <- c("year","occ")
  year_losses <- merge(year_losses, agg_losses, by = "year", all.x = TRUE)
  year_losses <- merge(year_losses, occ_losses, by = "year", all.x = TRUE)
  year_losses[is.na(year_losses)] <- 0
  
  # write out the year losses
  write.csv(year_losses, paste0(output_path, "02_year_losses/", "year_losses_prov_",adm2,".csv"), row.names = FALSE)
  
  
  # calculate exceedance probabilities at defined return periods
  ep_losses$agg <- round(quantile(year_losses$agg, ep_losses$pctile),0)
  ep_losses$occ <- round(quantile(year_losses$occ, ep_losses$pctile),0)
  
  # write out the exceedance probability curve
  write.csv(ep_losses, paste0(output_path, "03_ep_losses/", "ep_losses_prov_",adm2,".csv"), row.names = FALSE)
  
}



################################################################################
# Create RP loss curve for each target municipality
################################################################################


losses_by_adm3 <- read.csv(paste0(output_path, "01_events/" , "event_losses_by_target_mun.csv"))

nyears <- 10000
rp <- c(2,3,4,5,6,7,8,9,10,15,20,25,50,100,150,200,250,400,500,1000,5000,10000)
pctile <- 1 - 1/rp
ep_losses <- as.data.frame(cbind(rp, pctile))

for (j in 1:length(unique(losses_by_adm3$Mun_Code))) {
  adm3 <- unique(losses_by_adm3$Mun_Code)[j]
  losses_adm3 <- losses_by_adm3[which(losses_by_adm3$Mun_Code == adm3), ]
  
  # create year losses for years 1 to 10000, both aggregate and occurrence
  year_losses <- as.data.frame(c(1:nyears))
  colnames(year_losses) <- c("year")
  agg_losses <- aggregate(losses_adm3$loss_bldg, by = list(year = losses_adm3$year), FUN = sum)
  colnames(agg_losses) <- c("year","agg")
  occ_losses <- aggregate(losses_adm3$loss_bldg, by = list(year = losses_adm3$year), FUN = max)
  colnames(occ_losses) <- c("year","occ")
  year_losses <- merge(year_losses, agg_losses, by = "year", all.x = TRUE)
  year_losses <- merge(year_losses, occ_losses, by = "year", all.x = TRUE)
  year_losses[is.na(year_losses)] <- 0
  
  # write out the year losses
  write.csv(year_losses, paste0(output_path, "02_year_losses/by_mun/", "year_losses_mun_",adm3,".csv"), row.names = FALSE)
  
  
  # calculate exceedance probabilities at defined return periods
  ep_losses$agg <- round(quantile(year_losses$agg, ep_losses$pctile),0)
  ep_losses$occ <- round(quantile(year_losses$occ, ep_losses$pctile),0)
  
  # write out the exceedance probability curve
  write.csv(ep_losses, paste0(output_path, "03_ep_losses/by_mun/", "ep_losses_mun_",adm3,".csv"), row.names = FALSE)
  
}



################################################################################
# Check the total losses across all files
################################################################################

loss <- rep(0, length(input_files))
# loop through input files and sum losses
for (i in 1:length(input_files)) {
  
  input_file <- input_files[i]
  file <- gsub(".csv","",gsub("STORM_DATA_","",input_file))
  elt_raw <- read.csv(paste0(input_path, input_file))
  elt <- subset(elt_raw, select = -c(X, storm_id))
  
  # sum all losses in the ELT
  loss[i] <- sum(elt$DMG_predicted)
  
}

loss_total <- sum(loss)
loss_by_file <- cbind(loss,c(0:9))
colnames(loss_by_file) <- c("total_loss","file")
write.csv(loss_by_file, paste0(output_path, "01_events/", "loss_by_file.csv"), row.names = FALSE)


